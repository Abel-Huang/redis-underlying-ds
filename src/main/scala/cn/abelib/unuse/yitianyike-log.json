{"paragraphs":[{"title":"初始配置","text":"///** 定时任务在 6 分钟，每小时执行一次。避开整点, 如部分机器是 23:59:59 部分机器是 00:00:01 */\n\nimport java.net.URI\nimport java.security.MessageDigest\n\nimport org.apache.hadoop.fs.Path\nimport org.apache.hadoop.fs.qiniu.QiniuFileSystem\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext, SparkException}\nimport org.joda.time.DateTime\nimport org.joda.time.format.{DateTimeFormat, DateTimeFormatter}\nimport qiniu.ip17mon.Locator\n\nimport scala.annotation.switch\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.matching.Regex\n\n\n//  val ipfile = \"/usr/spark/3rd/17monipdb.dat\"\n // val ipfile = \"17monipdb.dat\"\n val ipfile = \"http://ok0n6ytwh.bkt.clouddn.com/17monipdb.dat\"\n val ak = \"55x\"\n val sk = \"d90\"\n\n  // 以上信息,除 case class Record 外, 开发环境 和 Zeppelin 代码不一样\n  // --------------------------------------------------------------------------------------------------\n\n  val bucket = \"qiniu://fusionlog\"\n  val path = \"http://fusionlog.qiniu.com\"\n  val devices = s\"${bucket}/yitianyike-devices-channel.csv\"\n\n//   sc.hadoopConfiguration.set(\"spark.sql.parquet.output.committer.class\", \"org.apache.spark.sql.parquet.DirectParquetOutputCommitter\")\n  sc.hadoopConfiguration.set(\"fs.qiniu.access.key\", ak)\n  sc.hadoopConfiguration.set(\"fs.qiniu.secret.key\", sk)\n  sc.hadoopConfiguration.set(\"fs.qiniu.bucket.domain\", path)\n  sc.hadoopConfiguration.set(\"fs.qiniu.file.retention\", \"55\")\n  \n  \n  val fs = new QiniuFileSystem();\n  fs.initialize(new URI(s\"$bucket/\"), sc.hadoopConfiguration)\n\n  def info(x: Any) = {\n    // println(x)\n  }\n  \n  def info1(x: Any) = {\n    println(x)\n  }\n\n  def warn(x: Any) = {\n    // Console.err.println(x)\n    println(x)\n  }\n\n  def deleteFile(delUrl: String, recursive: Boolean = true) = {\n    info(s\"开始删除文件 $delUrl* : ${new DateTime()}\")\n    fs.delete(new Path(delUrl), recursive)\n    info(s\"删除文件完成 $delUrl* : ${new DateTime()}\")\n  }","dateUpdated":"2017-02-19T21:36:23+0800","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221758_35940330","id":"20170215-105245_618538570","result":{"code":"SUCCESS","type":"TEXT","msg":"import java.net.URI\nimport java.security.MessageDigest\nimport org.apache.hadoop.fs.Path\nimport org.apache.hadoop.fs.qiniu.QiniuFileSystem\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext, SparkException}\nimport org.joda.time.DateTime\nimport org.joda.time.format.{DateTimeFormat, DateTimeFormatter}\nimport qiniu.ip17mon.Locator\nimport scala.annotation.switch\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.matching.Regex\nipfile: String = http://ok0n6ytwh.bkt.clouddn.com/17monipdb.dat\nak: String = 557TpseUM8ovpfUhaw8gfa2DQ0104ZScM-BTIcBx\nsk: String = d9xLPyreEG59pR01sRQcFywhm4huL-XEpHHcVa90\nbucket: String = qiniu://fusionlog\npath: String = http://fusionlog.qiniu.com\ndevices: String = qiniu://fusionlog/yitianyike-devices-channel.csv\nfs: org.apache.hadoop.fs.qiniu.QiniuFileSystem = org.apache.hadoop.fs.qiniu.QiniuFileSystem@2ab244f7\ninfo: (x: Any)Unit\ninfo1: (x: Any)Unit\nwarn: (x: Any)Unit\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\nwarning: Class org.joda.convert.ToString not found - continuing with a stub.\nwarning: Class org.joda.convert.ToString not found - continuing with a stub.\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\nwarning: Class org.joda.convert.ToString not found - continuing with a stub.\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\ndeleteFile: (delUrl: String, recursive: Boolean)Unit\n"},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:00+0800","dateFinished":"2017-02-20T12:30:29+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"title":"分表","text":"\n  // =============================================================\n  // 分表\n\n  val channelRegionUsercountType = \"ChannelRegionUsercount\"\n  val channelSystemUsercountType = \"ChannelSystemUsercount\"\n  val channelDownloadType = \"ChannelDownload\"\n  val channelTopKeyDownloadType = \"ChannelTopKeyDownload\"\n  val channelTopKeyFluxType = \"ChannelTopKeyFlux\"\n  val channelRestimeType = \"ChannelRestime\"\n  val channelStatusType = \"ChannelStatus\"\n\n  val types = List(channelRegionUsercountType, channelSystemUsercountType, channelDownloadType,\n    channelTopKeyDownloadType, channelTopKeyFluxType, channelRestimeType, channelStatusType)\n\n\n  // 注册主表，包含所有数据\n  def registerTable(parquetUrl: String, tableName: String) = {\n    info(s\" 开始注册 $parquetUrl 到表 $tableName: ${new DateTime()}\")\n    sqlContext.read.parquet(parquetUrl).registerTempTable(tableName)\n    info(s\" 注册 $parquetUrl 到表完成 $tableName: ${new DateTime()}\")\n  }\n\n  case class ChannelRegionUsercount(channel: String, model: String, nation: String,\n                                    province: String, city: String, userCount: Long, time: Int)\n\n  def alreadyAnalysed(writeUrl: String) = {\n    fs.exists(new Path(s\"$writeUrl/_SUCCESS\")) || fs.exists(new Path(s\"${writeUrl}_SUCCESS\"))\n  }\n\n  def createChannelRegionUsercount(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n        val channel = r.getString(0)\n        val model = r.getString(1)\n        val nation = r.getString(2)\n        val province = r.getString(3)\n        val city = r.getString(4)\n        val userCount = r.getLong(5)\n        val time = r.getInt(6)\n        ChannelRegionUsercount(channel, model, nation, province, city, userCount, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n\n  case class ChannelSystemUsercount(channel: String, system: String, userCount: Long, time: Int)\n\n  def createChannelSystemUsercount(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val system = r.getString(1)\n      val userCount = r.getLong(2)\n      val time = r.getInt(3)\n      ChannelSystemUsercount(channel, system, userCount, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  case class ChannelDownload(channel: String, count: Long, time: Int)\n\n  // 分渠道的总下载次数\n  def createChannelDownload(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val count = r.getLong(1)\n      val time = r.getInt(2)\n      ChannelDownload(channel, count, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  case class ChannelTopKeyDownload(channel: String, key: String, count: Long, time: Int)\n\n  // 分渠道的 top xxx 的下载次数\n  def createChannelTopKeyDownload(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val key = r.getString(1)\n      val count = r.getLong(2)\n      val time = r.getInt(3)\n      ChannelTopKeyDownload(channel, key, count, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  case class ChannelTopKeyFlux(channel: String, key: String, flux: Long, time: Int)\n\n  // 分渠道的 top xxx 的流量\n  def createChannelTopKeyFlux(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val key = r.getString(1)\n      val flux = r.getLong(2)\n      val time = r.getInt(3)\n      ChannelTopKeyFlux(channel, key, flux, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  case class ChannelRestime(channel: String, size: Long, restime: Long, time: Int)\n\n  // 下载的总大小、总耗时\n  def createChannelRestime(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val size = r.getLong(1)\n      val restime = r.getLong(2)\n      val time = r.getInt(3)\n      ChannelRestime(channel, size, restime, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  case class ChannelStatus(channel: String, code: Int, count: Long, time: Int)\n\n  // 渠道、状态码、状态码个数\n  def createChannelStatus(sql: String, writeUrl: String) = {\n    val reginrdd = sqlContext.sql(sql).map(r => {\n      val channel = r.getString(0)\n      val code = r.getInt(1)\n      val count = r.getLong(2)\n      val time = r.getInt(3)\n      ChannelStatus(channel, code, count, time)\n    })\n    (sqlContext.createDataFrame(reginrdd).write, writeUrl, sql)\n  }\n\n  def analyseTable(writeParquetUrlPrefix: String, timeSuffix: String, check: Boolean,\n                   typeSqls: Seq[(String, String, String, String)]): Int = {\n    val needAnalyses = typeSqls.filterNot(typeSqlUrlTable => {\n      check && alreadyAnalysed(s\"$writeParquetUrlPrefix/${typeSqlUrlTable._1}/$timeSuffix\")\n    })\n\n    if (needAnalyses.length > 0) {\n      needAnalyses.map(i => (i._3, i._4)).distinct.par.foreach(p => registerTable(p._1, p._2))\n\n      val tasks = needAnalyses.par.map(typeSql => {\n        val aType = typeSql._1\n        val sql = typeSql._2\n        val writeUrl = s\"$writeParquetUrlPrefix/$aType/$timeSuffix\"\n        deleteFile(writeUrl, true)\n        (aType: @switch) match {\n          case `channelRegionUsercountType` => createChannelRegionUsercount(sql, writeUrl)\n          case `channelSystemUsercountType` => createChannelSystemUsercount(sql, writeUrl)\n          case `channelDownloadType` => createChannelDownload(sql, writeUrl)\n          case `channelTopKeyDownloadType` => createChannelTopKeyDownload(sql, writeUrl)\n          case `channelTopKeyFluxType` => createChannelTopKeyFlux(sql, writeUrl)\n          case `channelRestimeType` => createChannelRestime(sql, writeUrl)\n          case `channelStatusType` => createChannelStatus(sql, writeUrl)\n        }\n      })\n\n      val msg = ArrayBuffer[String]()\n\n      tasks.par.foreach(w => {\n        val write = w._1\n        val writeUrl = w._2\n        val sql = w._3\n        info(s\"开始汇总表: $writeUrl : ${new DateTime()}\")\n        try {\n          write.parquet(writeUrl)\n          info(s\"汇总表完成: $writeUrl : ${new DateTime()}\")\n        } catch {\n          case e: Throwable => {\n            val s = s\"  汇总表失败: $write $sql to $writeUrl : ${new DateTime()}\"\n            msg += s\n            warn(s)\n            warn(e.getMessage)\n            warn(e.toString)\n            e.printStackTrace()\n          }\n        }\n      })\n\n      info1(s\"汇总表结束: $writeParquetUrlPrefix/*  */$timeSuffix : ${new DateTime()}, 失败信息: \\n ${msg.mkString(\"\\n\")}\")\n    }\n    needAnalyses.length\n  }\n\n  ////////     按每小时汇总\n  def hourParquetSuffix(subject: String) = s\"$bucket/parquet/$subject/hour\"\n\n  def createHourTable(domain: String, datetime: DateTime, check: Boolean = true) = {\n    val timeSuffix = datetime.toString(\"yyyy/MM/dd/HH\")\n    val time = datetime.getMillis / 1000 / 3600 * 3600\n    val parquetOriginUrl = s\"$bucket/parquet/${domain}/${timeSuffix}/*\"\n    val writeParquetUrlPrefix = hourParquetSuffix(domain)\n    val tableName = s\"ytyk_temp_hour_${time}\"\n\n    info(s\"开始汇总小时表: $parquetOriginUrl to $writeParquetUrlPrefix : $tableName  : ${new DateTime()}\\n\")\n\n\n    val typeSqls = Seq(\n      (channelRegionUsercountType, s\" select channel, model, nation, province, city, count(DISTINCT user) as userCount, \" +\n      s\" ${time} as time from ${tableName} where nation = '中国' group by  channel, model, nation, province, city \",\n        parquetOriginUrl, tableName),\n\n      (channelSystemUsercountType, s\" select channel, system, count(DISTINCT user) as userCount, ${time} as time \" +\n      s\" from ${tableName} where nation = '中国' group by  channel, system \", parquetOriginUrl, tableName),\n\n    // 带宽： 5 分钟流量 / 300 * 8 bps\n    // 小时带宽， 取小时内 5 分钟带宽的最大值\n    // 暂不做\n    // ChannelBandwith\n    // channel, bandwith, time\n\n    // 分渠道的总下载次数\n      (channelDownloadType, s\" select channel, count(1) as count, ${time} as time \" +\n      s\" from ${tableName} where nation = '中国' group by  channel \", parquetOriginUrl, tableName),\n\n    //    // 分渠道的 top 500 的下载次数\n    //    select  channel, key, count from (\n    //      select channel, key, count,  row_number() over(partition by channel order by count desc) idx  from (\n    //      select channel, key, count(1) as count\n    //        from ytyk where nation = '中国'  group by  channel, key\n    //    ) a ) b where idx < 5 order by channel, count desc\n      (channelTopKeyDownloadType, s\"select  channel, key, count, ${time} as time from ( \" +\n      \" select channel, key, count, row_number() over(partition by channel order by count desc) idx from ( \" +\n      s\" select channel, key, count(1) as count from ${tableName} where nation = '中国'  group by  channel, key\" +\n      \" ) a ) b where idx < 500 order by channel, count desc\", parquetOriginUrl, tableName),\n\n    // 分渠道的 top 500 的流量\n      (channelTopKeyFluxType,  s\"select  channel, key, flux, ${time} as time from ( \" +\n      \" select channel, key, flux, row_number() over(partition by channel order by flux desc) idx from ( \" +\n      s\" select channel, key, sum(size) as flux from ${tableName} where nation = '中国'  group by  channel, key\" +\n      \" ) a ) b where idx < 500 order by channel, flux desc\", parquetOriginUrl, tableName),\n\n    // 下载的总大小、总耗时\n      (channelRestimeType, s\" select  channel, sum(size) as totalsize, sum(restime) as totalrestime, ${time} as time \" +\n      s\" from ${tableName} where nation = '中国' group by  channel \", parquetOriginUrl, tableName),\n\n    // 渠道、状态码、状态码个数\n      (channelStatusType, s\" select channel, code, count(1), ${time} as time \" +\n      s\" from ${tableName} where nation = '中国' group by  channel, code \", parquetOriginUrl, tableName)\n    )\n\n    analyseTable(writeParquetUrlPrefix, timeSuffix, check, typeSqls)\n  }\n\n  /////////  按已汇总数据统计\n\n  def createTable(domain: String, fromDateType: String, time: Int, timeSuffix: String, writeParquetUrlPrefix: String,\n                  tableSuffix: String, check: Boolean) = {\n    val pPrefix = s\"${bucket}/parquet/${domain}/${fromDateType}\"\n\n    val typeSqlParquentTables = Seq(\n      (channelRegionUsercountType, s\" select channel, model, nation, province, city, sum(userCount) as userCount, \" +\n      s\" ${time} as time from ${channelRegionUsercountType}${tableSuffix} group by channel, model, nation, province, city \",\n       s\"$pPrefix/$channelRegionUsercountType/${timeSuffix}/*\", s\"$channelRegionUsercountType$tableSuffix\"),\n\n      (channelSystemUsercountType, s\" select  channel, system, sum(userCount) as userCount,  ${time} as time \" +\n      s\"from ${channelSystemUsercountType}${tableSuffix} group by  channel, system \",\n        s\"$pPrefix/$channelSystemUsercountType/${timeSuffix}/*\", s\"$channelSystemUsercountType$tableSuffix\"),\n\n    // 带宽： 5 分钟流量 / 300 * 8 bps\n    // 小时带宽， 取小时内 5 分钟带宽的最大值\n    // 暂不做\n    // ChannelBandwith\n    // channel, bandwith, time\n\n    // 分渠道的总下载次数\n      (channelDownloadType, s\" select   channel, sum(count) as count,  ${time} as time \" +\n      s\"from ${channelDownloadType}${tableSuffix} group by  channel \",\n        s\"$pPrefix/$channelDownloadType/${timeSuffix}/*\", s\"$channelDownloadType$tableSuffix\"),\n\n    //    // 分渠道的 top 500 的下载次数\n    //        select  channel, key, count from (\n    //    select channel, key, count,  row_number() over(partition by channel order by count desc) idx  from (\n    //      select channel, key, sum(count) as count\n    //        from ChannelTopKeyDownload_ytyk_temp_day_1486944000  group by  channel, key\n    //    ) a ) b where idx < 5 order by channel, count desc\n      (channelTopKeyDownloadType, s\"select  channel, key, count, ${time} as time from ( \" +\n      \" select channel, key, count, row_number() over(partition by channel order by count desc) idx from ( \" +\n      s\" select channel, key, sum(count) as count from ${channelTopKeyDownloadType}${tableSuffix} group by  channel, key\" +\n      \" ) a ) b where idx < 500 order by channel, count desc\",\n        s\"$pPrefix/$channelTopKeyDownloadType/${timeSuffix}/*\", s\"$channelTopKeyDownloadType$tableSuffix\"),\n\n    // 分渠道的 top 500 的流量\n      (channelTopKeyFluxType, s\"select  channel, key, flux, ${time} as time from ( \" +\n      \" select channel, key, flux, row_number() over(partition by channel order by flux desc) idx from ( \" +\n      s\" select channel, key, sum(flux) as flux from ${channelTopKeyFluxType}${tableSuffix} group by  channel, key\" +\n      \" ) a ) b where idx < 500 order by channel, flux desc\",\n        s\"$pPrefix/$channelTopKeyFluxType/${timeSuffix}/*\", s\"$channelTopKeyFluxType$tableSuffix\"),\n\n    // 下载的总大小、总耗时\n      (channelRestimeType, s\" select  channel, sum(size) as size, sum(restime) as restime, ${time} as time \" +\n      s\" from ${channelRestimeType}${tableSuffix} group by  channel \",\n        s\"$pPrefix/$channelRestimeType/${timeSuffix}/*\", s\"$channelRestimeType$tableSuffix\"),\n\n    // 渠道、状态码、状态码个数\n      (channelStatusType, s\" select channel, code, sum(count) as count, ${time} as time \" +\n      s\" from ${channelStatusType}${tableSuffix} group by  channel, code \",\n        s\"$pPrefix/$channelStatusType/${timeSuffix}/*\", s\"$channelStatusType$tableSuffix\")\n    )\n\n    analyseTable(writeParquetUrlPrefix, timeSuffix, check, typeSqlParquentTables)\n  }\n\n  ////////     小时汇总数据汇总为天数据\n\n  def dayParquetSuffix(subject: String) = s\"$bucket/parquet/$subject/day\"\n\n  def createDayTable(domain: String, datetime: DateTime, check: Boolean = true) = {\n    val timeSuffix = datetime.toString(\"yyyy/MM/dd\")\n    val time = datetime.getMillis / 1000 / 86400 * 86400\n    val writeParquetUrlPrefix = dayParquetSuffix(domain)\n\n    val dayTableSuffix = s\"_ytyk_temp_day_${time}\"\n\n    createTable(domain, \"hour\", time.toInt, timeSuffix, writeParquetUrlPrefix, dayTableSuffix, check)\n  }\n\n  ////////     天汇总数据汇总为月数据\n\n  def monthParquetSuffix(subject: String) = s\"$bucket/parquet/$subject/month\"\n\n  def createMonthTable(domain: String, datetime: DateTime, check: Boolean = true) = {\n    val timeSuffix = datetime.toString(\"yyyy/MM/dd\")\n    val time = new DateTime(datetime.getYear, datetime.getMonthOfYear, 0).getMillis / 1000\n    val writeParquetUrlPrefix = monthParquetSuffix(domain)\n\n    val tableSuffix = s\"_ytyk_month_day_${time}\"\n\n    createTable(domain, \"day\", time.toInt, timeSuffix, writeParquetUrlPrefix, tableSuffix, check)\n  }\n\n\n  // =============================================================\n","dateUpdated":"2017-02-19T21:36:24+0800","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221761_119430841","id":"20170215-105351_1498666280","result":{"code":"SUCCESS","type":"TEXT","msg":"channelRegionUsercountType: String = ChannelRegionUsercount\nchannelSystemUsercountType: String = ChannelSystemUsercount\nchannelDownloadType: String = ChannelDownload\nchannelTopKeyDownloadType: String = ChannelTopKeyDownload\nchannelTopKeyFluxType: String = ChannelTopKeyFlux\nchannelRestimeType: String = ChannelRestime\nchannelStatusType: String = ChannelStatus\ntypes: List[String] = List(ChannelRegionUsercount, ChannelSystemUsercount, ChannelDownload, ChannelTopKeyDownload, ChannelTopKeyFlux, ChannelRestime, ChannelStatus)\nregisterTable: (parquetUrl: String, tableName: String)Unit\ndefined class ChannelRegionUsercount\nalreadyAnalysed: (writeUrl: String)Boolean\ncreateChannelRegionUsercount: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelSystemUsercount\ncreateChannelSystemUsercount: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelDownload\ncreateChannelDownload: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelTopKeyDownload\ncreateChannelTopKeyDownload: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelTopKeyFlux\ncreateChannelTopKeyFlux: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelRestime\ncreateChannelRestime: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\ndefined class ChannelStatus\ncreateChannelStatus: (sql: String, writeUrl: String)(org.apache.spark.sql.DataFrameWriter, String, String)\n<console>:109: warning: could not emit switch for @switch annotated match\n               (aType: @switch) match {\n                        ^\nanalyseTable: (writeParquetUrlPrefix: String, timeSuffix: String, check: Boolean, typeSqls: Seq[(String, String, String, String)])Int\nhourParquetSuffix: (subject: String)String\ncreateHourTable: (domain: String, datetime: org.joda.time.DateTime, check: Boolean)Int\ncreateTable: (domain: String, fromDateType: String, time: Int, timeSuffix: String, writeParquetUrlPrefix: String, tableSuffix: String, check: Boolean)Int\ndayParquetSuffix: (subject: String)String\ncreateDayTable: (domain: String, datetime: org.joda.time.DateTime, check: Boolean)Int\nmonthParquetSuffix: (subject: String)String\ncreateMonthTable: (domain: String, datetime: org.joda.time.DateTime, check: Boolean)Int\n"},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:02+0800","dateFinished":"2017-02-20T12:30:37+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"title":"数据清洗","text":"\n  case class Record(user: String, ip: String, nation: String, province: String, city: String,\n                    restime: Int, time: Long, code: Int, size: Long,\n                    system: String, channel: String, device: String, model: String, build: String,\n                    key: String, suffix: String)\n\n\n  val devicesM = sc.textFile(devices).map(x => {\n    val s = x.split(\",\");\n    (s(0), s(1))\n  }).collectAsMap()\n\n  def getChannelByDevice(device: String): String = {\n    devicesM.getOrElse(device, \"OTHOER\")\n  }\n\n  def stripChars(s: String, ch: String) = s filterNot (ch contains _)\n\n  // AndroidDownloadManager/5.1.1+(Linux;+U;+Android+5.1.1;+OPPO+R9+Plusm+A+Build/LMY47V)\n  // Dalvik/2.1.0+(Linux;+U;+Android+5.1.1;+NX529J+Build/LMY47V)\n  // Dalvik/2.1.0+(Linux;+U;+Android+5.1.1;+NX523J_V1+Build/LMY47V)\n  // Dalvik/2.1.0+(Linux;+U;+Android+6.0.1;+vivo+Y55A+Build/MMB29M)\n  // AndroidDownloadManager/5.1+(Linux;+U;+Android+5.1;+OPPO+R9m+Build/LMY47I)\n  // ua 也包含其它字符\n  // -\n  // Java/1.7.0_09\n  // Go-http-client/1.1\n  // VAYXXLWZIKRFDGFHPOXDNHJTDLTNBTV\n  // (\"Android 6.0.1\", \"vivo Y55A\", \"Build/MMB29M\")\n  //\n  //  val firm = driver._1    Android 5.1\n  //  val device = driver._2   OPPO R9m\n  //  val rom = driver._3       Build/LMY47I\n  //  ---------------------\n  //  system Android 5.1\n  //  device oppo\n  //  model A59m\n  //  build LMY47\n  // 27.129.192.99 - 96 [03/Jan/2017:23:30:37 +0800] \"GET http://7xna64.com2.z0.glb.qiniucdn.com/FqiqaB9Dgea0EluvDtfS9RPMnIll.jpg?imageView2/2/w/1080/h/1920&e=1472744999&token=Q-hCY0VbL4F6NTX3TgRvE_T3vcpNEo2Gr3S9RA-b:gjgcZwEQh3IradE00dse88e8zww= HTTP/1.1\" 401 608 \"-\" \"Mozilla/5.0+(Linux;+U;+Android+6.0;+zh-cn;GIONEE-GN8002/GIONEE-GN8002+Build/IMM76D)+AppleWebKit534.30(KHTML,like+Gecko)Version/4.0+Mobile+Safari/534.30+Id/AAC3010D557B0FC202D98648E58A517C+RV/5.0.16\"\n  def parseUa(ua: String): (String, String, String, String, String) = {\n    try {\n      val uas = ua.split(\";\")\n\n      //Linux;+U;+Android+5.1.1;+zh-cn;GiONEE-M3S/M3S+Build/IMM76D\n      //remove +/-/space chars from  \"+Android+5.1.1\"\n      var system = stripChars(uas(2), \"'\\\"+- \")\n      if (!\"\"\"(Android|Xhrome)[0-9.]*\"\"\".r.unapplySeq(system).isDefined) {\n        system = \"Unknown\"\n      }\n\n      //get GiONEE-M3S/M3S+Build/IMM76D\n      //+OPPO+R\"\n      //NX531J Build/MMB29M\n      //+HTC+D816w+Build/MRA58K\n      val deviceModelBuild = uas.last.trim()\n      val deviceModelBuilds = deviceModelBuild.replaceAll(\"[+-]\", \" \").replaceAll(\"\\\\s{2,}\", \" \").trim.split(' ')\n      val device = deviceModelBuilds(0)\n\n      var model = device\n      var build = \"\"\n\n      //NX531J Build/MMB29M\n      if (deviceModelBuilds.length > 1 && !deviceModelBuilds(1).contains(\"Build\")) {\n        val end = deviceModelBuilds.length - 1\n        model = deviceModelBuilds.slice(1, end).mkString(\"+\")\n      }\n\n      if (deviceModelBuild.contains(\"Build\")) {\n        build = deviceModelBuild.split(\"Build/\").last\n      }\n\n      val channel = getChannelByDevice(device)\n\n      return (system, channel, device, model, build)\n    } catch {\n      case e: Exception => {\n        info(s\"wrong user-agent==>:\", ua)\n        return (\"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\")\n      }\n    }\n  }\n\n\n  def parseToDate(dateParser: DateTimeFormatter, line: String): Long = {\n    dateParser.parseDateTime(line.substring(line.indexOf(\"[\") + 1, line.indexOf(\"]\"))).getMillis / 1000\n  }\n\n  def parseRegion(ipParser: Locator, ip: String) = {\n    /**\n      *\n      * [中国, 河南, 新乡, ]\n      * [中国, 贵州, 黔南布依族苗族自治州, ]\n      * [中国, 广东, 中山, ]\n      * [中国, 安徽, 芜湖, ]\n      * [共享地址, 共享地址, , ]\n      *\n      **/\n    val ipinfo = ipParser.find(ip)\n    //   if (\"共享地址\".equalsIgnoreCase(ipinfo(0))) throw new Exception(s\"${ip} is a '共享地址'\")\n    (ipinfo.country, ipinfo.state, ipinfo.city)\n  }\n\n  def parseToKey(url: String) = {\n    // https://a 至少有 9 个字符\n    val l = url.indexOf(\"?\", 9);\n    val end = if (l > 0) l else url.length()\n    url.substring(url.indexOf(\"/\", 9) + 1, end)\n  }\n\n  def hash(sha1Digester: MessageDigest, s: String): String = {\n    sha1Digester.digest(s.getBytes).map(\"%02x\".format(_)).mkString\n  }\n\n\n  def mixtureUser(sha1Digester: MessageDigest, ip: String, ua: String) = {\n    hash(sha1Digester, ip + \":\" + ua)\n  }\n\n  def parseToSuffix(key: String) = {\n    val s = key.lastIndexOf('.')\n    if (s < 0) {\n      \"\"\n    } else {\n      key.substring(s, key.length)\n    }\n  }\n\n\n  // 106.18.21.156 - 282 [03/Jan/2017:23:30:14 +0800] \"GET http://7xna64.com2.z0.glb.qiniucdn.com/Fjm_mLtcPN3DbTtLpywOmX5gq9cl.jpg?imageView2/2/w/1080/h/1920&e=1483545599&token=Q-hCY0VbL4F6NTX3TgRvE_T3vcpNEo2Gr3S9RA-b:ffDUURujc65VJLj1mKdGDMOrhIg= HTTP/1.1\" 200 478114 \"-\" \"AndroidDownloadManager/5.1+(Linux;+U;+Android+5.1;+OPPO+R9m+Build/LMY47I)\"\n  // 139.148.121.96 - 248 [03/Jan/2017:23:30:11 +0800] \"GET http://7xna64.com2.z0.glb.qiniucdn.com/FiU3bxGjI6PutwVphDQQihBgP0uw.jpg?imageView2/2/w/1080/h/1920&e=1483545599&token=Q-hCY0VbL4F6NTX3TgRvE_T3vcpNEo2Gr3S9RA-b:1wKdyBO_iYMQh7_MBqGcifYQX50= HTTP/1.1\" 200 552867 \"-\" \"AndroidDownloadManager/5.1.1+(Linux;+U;+Android+5.1.1;+OPPO+R9+Plusm+A+Build/LMY47V)\"\n  // 220.178.4.219 - 1 [03/Jan/2017:23:30:35 +0800] \"GET http://7xna64.com2.z0.glb.qiniucdn.com/FiwmSuSIuu981zLWENSCOJvIoj2P.jpg?imageView2/2/w/1080/h/1920&e=1483592399&token=Q-hCY0VbL4F6NTX3TgRvE_T3vcpNEo2Gr3S9RA-b:vsvEgQcb8-cU3BDLNp6sLCG72DI= HTTP/1.1\" 200 456693 \"-\" \"Dalvik/2.1.0+(Linux;+U;+Android+5.1.1;+NX529J+Build/LMY47V)\"\n  def parse(dateParser: DateTimeFormatter, sha1Digester: MessageDigest, ipParser: Locator, line: String): Record = {\n    try {\n      val as = line.split(\" \")\n\n      val ip = as(0)\n      val restime = as(2).toInt\n      val time = parseToDate(dateParser, line)\n\n      val code = as(8).toInt\n      val size = as(9).toLong\n\n      val pattern = new Regex(\"Linux.*U[^)]*\")\n      val ua = (pattern findAllIn line).mkString\n\n      val region = parseRegion(ipParser, ip)\n      val nation = region._1\n      val province = region._2\n      val city = region._3\n\n      val driver = parseUa(ua)\n      val system = driver._1\n      val channel = driver._2\n      val device = driver._3\n      val model = driver._4\n      val build = driver._5\n\n      val user = mixtureUser(sha1Digester, ip, ua)\n\n      val key = parseToKey(as(6))\n\n      val suffix = parseToSuffix(key)\n\n      return Record(user, ip, nation, province, city,\n        restime, time, code, size,\n        system, channel, device, model, build, key, suffix)\n    } catch {\n      case e: Exception => {\n        e.printStackTrace()\n        info(s\"wrong line: ${line}\")\n        return null\n      }\n    }\n  }\n\n  def parseLog(url: String, markUrl: String) = {\n    info(url, markUrl)\n    val rdd = sc.textFile(url)\n    val infordd = rdd.mapPartitions(iter => {\n      val ipParser = Locator.loadFromNet(ipfile)\n      val dateParser = DateTimeFormat.forPattern(\"dd/MMM/yyyy:HH:mm:ss Z\")\n      val sha1Digester = MessageDigest.getInstance(\"SHA1\")\n      iter.map(parse(dateParser, sha1Digester, ipParser, _)).filter(_ != null)\n    })\n\n    info(\"==========+++++++++++++\")\n    // 本地运行此行代码,报 612 ,原因待查。Zeppelin 上正常\n    sqlContext.createDataFrame(infordd).write.parquet(markUrl)\n    info(\"++++++++++=+++++++++++++\")\n\n    //    val df = sqlContext.createDataFrame(infordd)\n    //    df.registerTempTable(\"fusion\")\n    //    df.sqlContext.sql(\"select * from fusion limit 10\").collect().foreach(info)\n    //    df.sqlContext.sql(\"select count(1) from fusion\").collect().foreach(info)\n  }\n\n  def parseAndStore(domain: String, datetime: DateTime, check: Boolean = true) = {\n    info(\"start at: \", new DateTime())\n\n    val date = datetime.toString(\"yyyy/MM/dd/HH\")\n\n    val r = s\"$bucket/v2/${domain}_${datetime.toString(\"yyyy-MM-dd-HH\")}*\"\n    val w = s\"$bucket/parquet/$domain/$date/\"\n\n    if (check && fs.exists(new Path(s\"${w}_SUCCESS\"))) {\n      info(s\"$w/_SUCCESS is exists, ${r} has been parsed.: ${new DateTime()}\")\n      false\n    } else {\n      // 删除上次生成文件，以及成功标识符\n      info(s\" ==== start to Parse $r To parquet: $w : ${new DateTime()}====\")\n      deleteFile(w)\n      parseLog(r, w)\n      info(s\" ==== Parsed over $r To parquet: $w : ${new DateTime()}====\")\n      true\n    }\n  }\n\n","dateUpdated":"2017-02-19T21:36:24+0800","config":{"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221762_120585088","id":"20170215-105436_522775038","result":{"code":"SUCCESS","type":"TEXT","msg":"defined class Record\ndevicesM: scala.collection.Map[String,String] = Map(iΡhone -> OTHER, GN5002 -> GIONEE, R7Plus -> OPPO, GN5001L -> GIONEE, htc6535lvw -> HTC, M5L -> GIONEE, z11无边框旗舰手机 -> NUBIA, 1603 -> OTHER, q5plus -> OTHER, SOP -> OTHER, 土豪金 -> OTHER, lt_c4000 -> OTHER, F8909 -> OTHER, Lovme_T16 -> OTHER, 1505_A01 -> QIKU, e910_hualiban -> OTHER, AoleDior -> OTHER, 特制版 -> OTHER, meizu -> OTHER, pioneer -> OTHER, F103 -> GIONEE, vivo -> VIVO, n5117 -> OPPO, NuoFei -> OTHER, k1001 -> OTHER, 魅族4g -> OTHER, KONKA -> OTHER, m3 -> GIONEE, gn715 -> GIONEE, GN3002 -> GIONEE, t1 -> OTHER, go -> OTHER, nx523j_v1 -> NUBIA, 7S -> GIONEE, gn9012 -> GIONEE, Philips -> OTHER, gn9006 -> GIONEE, 国民小污帝 -> OTHER, HONOR -> OTHER, OPPOR9 -> OPPO, guomi -> OTHER, v9180 -> OTHER, GNS6 -> GIONEE, oneplus -> OTHER, MILLED -> ...getChannelByDevice: (device: String)String\nstripChars: (s: String, ch: String)String\nparseUa: (ua: String)(String, String, String, String, String)\nparseToDate: (dateParser: org.joda.time.format.DateTimeFormatter, line: String)Long\nparseRegion: (ipParser: qiniu.ip17mon.Locator, ip: String)(String, String, String)\nparseToKey: (url: String)String\nhash: (sha1Digester: java.security.MessageDigest, s: String)String\nmixtureUser: (sha1Digester: java.security.MessageDigest, ip: String, ua: String)String\nparseToSuffix: (key: String)String\nparse: (dateParser: org.joda.time.format.DateTimeFormatter, sha1Digester: java.security.MessageDigest, ipParser: qiniu.ip17mon.Locator, line: String)Record\nparseLog: (url: String, markUrl: String)Unit\nparseAndStore: (domain: String, datetime: org.joda.time.DateTime, check: Boolean)Boolean\n"},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:30+0800","dateFinished":"2017-02-20T12:30:42+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"title":"调度","text":"\n  def execHour(domain: String, dateTime: DateTime, msg: ArrayBuffer[String], check: Boolean=true) = {\n    info(s\"\\n\\n========================  ${dateTime}\")\n    try {\n      info(\"parseAndStore: \", domain, dateTime, check)\n      parseAndStore(domain, dateTime, check)\n      info(\"createHourTable: \", domain, dateTime, check)\n      createHourTable(domain, dateTime, check)\n    } catch {\n      case e: SparkException => {\n        if (e.getCause != null) {\n          msg += s\" ${domain}, ${dateTime}, ${e}: ${e.getCause.getMessage}: ${new DateTime()}\"\n        } else {\n          msg +=  s\" ${domain}, ${dateTime}, ${e.getMessage}: ${new DateTime()}\"\n        }\n        throw e\n      }\n      case e: Throwable => {\n        msg +=  s\" ${domain}, ${dateTime}, ${e.getMessage}: ${new DateTime()}\"\n        throw e\n      }\n    }\n  }\n\n\n  // redoHour(Seq(new DateTime(2017, 2, 13, 19, 5), new DateTime(2017, 2, 13, 20, 5)))\n  def execHours(domain: String, datetimes: Seq[DateTime], check: Boolean = true) = {\n    val msg = ArrayBuffer[String]()\n    var ex: Throwable = null\n\n    datetimes.par.foreach(dateTime => {\n      try {\n        execHour(domain, dateTime, msg, check)\n      } catch {\n        case e: Throwable => {\n          if (ex == null) {\n            ex = e\n          }\n        }\n      }\n    })\n\n    if (msg.length > 0) {\n      val s = \"\\n parsing or create diff table failed: \\n \" + msg.mkString(\",\\n \")\n      if(ex != null) {\n        ex.printStackTrace()\n        throw new Exception(s, ex)\n      }\n    }\n  }\n\n\n  def cleanHour(domain: String, datetimes: Seq[DateTime]) = {\n    val msg = ArrayBuffer[String]()\n\n    try{\n      datetimes.par.foreach(datetime => {\n        info(\"clean parquet parseAndStore: \", domain, datetime)\n        val date = datetime.toString(\"yyyy/MM/dd/HH\")\n        val w = s\"$bucket/parquet/$domain/$date/\"\n        deleteFile(s\"${w}_SUCCESS\", false)\n\n        info(\"clean hour table: \", domain, datetime)\n        val timeSuffix = datetime.toString(\"yyyy/MM/dd/HH\")\n        val writeParquetUrlPrefix = hourParquetSuffix(domain)\n\n        types.par.foreach(t => {\n          val f = s\"$writeParquetUrlPrefix/$t/$timeSuffix/_SUCCESS\"\n          deleteFile(f, false)\n        })\n      })\n    } catch {\n      case e: SparkException => {\n        if (e.getCause != null) {\n          msg += s\" ${domain}, ${e}: ${e.getCause.getMessage}: ${new DateTime()}\"\n        } else {\n          msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        }\n        // throw e\n      }\n      case e: Throwable => {\n        msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        // throw e\n      }\n    }\n  }\n\n\n  def cleanDay(domain: String, datetimes: Seq[DateTime]) = {\n    val msg = ArrayBuffer[String]()\n\n    try{\n      datetimes.par.foreach(datetime => {\n        info(\"clean day table: \", domain, datetime)\n\n        val timeSuffix = datetime.toString(\"yyyy/MM/dd\")\n        val writeParquetUrlPrefix = dayParquetSuffix(domain)\n\n        types.par.foreach(t => {\n          val f = s\"$writeParquetUrlPrefix/$t/$timeSuffix/_SUCCESS\"\n          deleteFile(f, false)\n        })\n      })\n    } catch {\n      case e: SparkException => {\n        if (e.getCause != null) {\n          msg += s\" ${domain}, ${e}: ${e.getCause.getMessage}: ${new DateTime()}\"\n        } else {\n          msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        }\n        // throw e\n      }\n      case e: Throwable => {\n        msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        // throw e\n      }\n    }\n  }\n\n\n\n  def cleanMonth(domain: String, datetimes: Seq[DateTime]) = {\n    val msg = ArrayBuffer[String]()\n\n    try{\n      datetimes.par.foreach(datetime => {\n        info(\"clean month table: \", domain, datetime)\n\n        val timeSuffix = datetime.toString(\"yyyy/MM/dd\")\n        val writeParquetUrlPrefix = monthParquetSuffix(domain)\n\n        types.par.foreach(t => {\n          val f = s\"$writeParquetUrlPrefix/$t/$timeSuffix/_SUCCESS\"\n          deleteFile(f, false)\n        })\n      })\n    } catch {\n      case e: SparkException => {\n        if (e.getCause != null) {\n          msg += s\" ${domain}, ${e}: ${e.getCause.getMessage}: ${new DateTime()}\"\n        } else {\n          msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        }\n        // throw e\n      }\n      case e: Throwable => {\n        msg +=  s\" ${domain},  ${e.getMessage}: ${new DateTime()}\"\n        // throw e\n      }\n    }\n  }\n\n\n  def start(): Unit = {\n    val domain = \"7xna64.com2.z0.glb.qiniucdn.com\"\n\n    try{\n      val now = new DateTime()\n      if ((now.getDayOfMonth == 3 && (now.getHourOfDay == 0 || now.getHourOfDay == 2)) ||\n        (now.getDayOfMonth == 4 && (now.getHourOfDay == 0 || now.getHourOfDay == 2))) {\n        info(\"createMonthTable: \", domain, now.plusDays(-10))\n        // 上个月,具体天不重要\n        createMonthTable(domain, now.plusDays(-10))\n      }\n    } catch {\n      case e: Throwable => {\n        e.printStackTrace()\n        // throw e\n      }\n    }\n\n    try {\n      val now = new DateTime()\n      if (now.getHourOfDay == 16 || now.getHourOfDay == 18) {\n        info(\"createDayTable: \", domain, now.plusHours(-36))\n        // 昨天,具体小时不重要\n        // createDayTable(domain, now.plusHours(-24 - 10))\n      }\n      if (now.getHourOfDay == 20 || now.getHourOfDay == 22) {\n        info(\"createDayTable: \", domain, now.plusHours(-36))\n        // 前天,具体小时不重要\n        // createDayTable(domain, now.plusHours(-48 - 10))\n      }\n    } catch {\n      case e: Throwable => {\n        e.printStackTrace()\n        // throw e\n      }\n    }\n\n    try {\n      val now = new DateTime()\n      val datetimes = (8).to(19).map(j => {\n        val i = -j\n        info(s\"\\n\\n============ ${i}\")\n        now.plusHours(i)\n      })\n      datetimes.grouped(6).foreach(v => {\n        v.foreach(info)\n        try{\n          execHours(domain, v, true)\n        } catch {\n          case e: Throwable => {\n            e.printStackTrace()\n            // throw e\n          }\n        }\n        info(\"task 'start' have been done\")\n      })\n    } catch {\n      case e: Throwable => {\n        e.printStackTrace()\n        // throw e\n      }\n    }\n    info(\"done\")\n  }\n\n  val line = new DateTime(2017, 2, 10, 17, 30)\n  val line2 = new DateTime(2017, 2, 14, 0, 0)\n\n  def start2() = {\n    val domain = \"7xna64.com2.z0.glb.qiniucdn.com\"\n    val now = new DateTime()\n\n    val datetimes = (19).to(400).map(i => {\n      now.plusHours(-i)\n    }).filter(d => d.getMillis > line.getMillis && d.getMillis < line2.getMillis)\n\n    datetimes.grouped(6).foreach(v => {\n      v.foreach(info)\n      try{\n        execHours(domain, v, true)\n      } catch {\n        case e: Throwable => {\n          e.printStackTrace()\n          // throw e\n        }\n      }\n    })\n    info(\"done\")\n  }\n\n  def start3() = {\n    val domain = \"7xna64.com2.z0.glb.qiniucdn.com\"\n    val now = new DateTime()\n    info(now.plusHours(-430))\n    val datetimes = (-430).to(-18).map(i => {\n      now.plusHours(i)\n    }).filter(d => d.getMillis > line.getMillis && d.getMillis < line2.getMillis)\n\n    datetimes.grouped(10).foreach(v => {\n      v.foreach(info)\n      try{\n        execHours(domain, v, true)\n      } catch {\n        case e: Throwable => {\n          e.printStackTrace()\n          // throw e\n        }\n      }\n    })\n    info(\"done\")\n  }","dateUpdated":"2017-02-19T21:36:24+0800","config":{"lineNumbers":true,"enabled":true,"title":true,"tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221763_120200339","id":"20170215-105454_448614723","result":{"code":"SUCCESS","type":"TEXT","msg":"execHour: (domain: String, dateTime: org.joda.time.DateTime, msg: scala.collection.mutable.ArrayBuffer[String], check: Boolean)Int\nexecHours: (domain: String, datetimes: Seq[org.joda.time.DateTime], check: Boolean)Unit\ncleanHour: (domain: String, datetimes: Seq[org.joda.time.DateTime])Any\ncleanDay: (domain: String, datetimes: Seq[org.joda.time.DateTime])Any\ncleanMonth: (domain: String, datetimes: Seq[org.joda.time.DateTime])Any\nstart: ()Unit\nline: org.joda.time.DateTime = 2017-02-10T17:30:00.000+08:00\nline2: org.joda.time.DateTime = 2017-02-14T00:00:00.000+08:00\nstart2: ()Unit\nstart3: ()Unit\n"},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:37+0800","dateFinished":"2017-02-20T12:30:44+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"title":"执行入口","text":"info(\"---------------\")\ninfo(s\"========= start at ${new DateTime()}\")\nstart()\ninfo(s\"========= end at ${new DateTime()}\")","dateUpdated":"2017-02-19T21:36:34+0800","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221764_118276594","id":"20170215-105513_732992286","result":{"code":"SUCCESS","type":"TEXT","msg":"汇总表结束: qiniu://fusionlog/parquet/7xna64.com2.z0.glb.qiniucdn.com/hour/*  */2017/02/20/04 : 2017-02-20T12:55:50.176+08:00, 失败信息: \n \n"},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:43+0800","dateFinished":"2017-02-20T12:55:50+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:218"},{"dateUpdated":"2017-02-19T21:36:24+0800","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1487497221764_118276594","id":"20170215-105532_1616845847","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-02-19T17:40:21+0800","dateStarted":"2017-02-20T12:30:45+0800","dateFinished":"2017-02-20T12:55:50+0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:219"}],"name":"yitianyike-log-36","id":"2C9EWRM9R","angularObjects":{"2CBFZP7W2:shared_process":[],"2CBV3W7R1:shared_process":[],"2CBG5WYHA:shared_process":[],"2CBE74Y5J:shared_process":[]},"config":{"looknfeel":"default","cron":"0 30 0/1 * * ?","releaseresource":true},"info":{}}